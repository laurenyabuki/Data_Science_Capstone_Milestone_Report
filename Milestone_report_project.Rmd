---
title: "Milestone_report_project"
author: "Lauren Yabuki"
date: "13/11/2020"
output: html_document
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Loading libraries
library(tm)
library(RWeka)
library(SnowballC) # important for the wordcloud package use
library(wordcloud)
library (stringi) # string/text manipulation
library(dplyr)
library(rvest) # reading html
library(xml2)
```
## Data aquisition

Download
```{r}
if(!file.exists("./final/en_US/en_US.blogs.txt") &&
   !file.exists("./final/en_US/en_US.news.txt") && 
    !file.exists("./final/en_US/en_US.twitter.txt")){
  URL <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
  download.file(URL, destfile="Coursera-SwiftKey.zip")
  unzip(zipfile="Coursera-SwiftKey.zip")
}
```
Opening conections and reading txt files
```{r warning=FALSE}
## twitter
con_twitter <- file("./final/en_US/en_US.twitter.txt")
twitter_raw <- readLines(con_twitter, encoding = "UTF-8", skipNul = TRUE)
close(con_twitter) #always close conection

## news
con_news <- file("./final/en_US/en_US.news.txt",open="r")
news_raw <- readLines(con_news, encoding = "UTF-8", skipNul = TRUE) 
close(con_news)

## blogs
con_blogs<-file("./final/en_US/en_US.blogs.txt", open="r") #r= Open for reading in text mode.
blogs_raw <- readLines(con_blogs, encoding = "UTF-8", skipNul = TRUE) 
close(con_blogs)

rm(con_blogs,con_news,con_twitter)
```

## Summary of the files
```{r}
# Word counts

words_twitter<-sum(stri_count_boundaries(twitter_raw, type="word"))
words_blog<-sum(stri_count_boundaries(blogs_raw, type="word"))
words_news<-sum(stri_count_boundaries(news_raw, type="word"))

# Summary of the files (lines and words counts)
files_summary<- data.frame(files=c("twitter","blogs", "news"), lines=c(length(twitter_raw),
length(blogs_raw),length(news_raw)), words=c(words_twitter,words_blog,words_news))
files_summary
```
# The twitter files has almost two million and four hundred thousand lines, the file representing blogs data has about nine hundred thousand lines and, the news has about seventy seven thousand lines.

## Data processing

Before processing the data, it is convenient to remove weird characters first. For that, data is converted from codepage Latin to ASCII.
```{r message=FALSE, warning=FALSE}
twitter_clean <- iconv(twitter_raw, 'UTF-8', 'ASCII', "byte")
blogs_clean<- iconv(blogs_raw, 'UTF-8', 'ASCII', "byte")
news_clean <- iconv(news_raw, 'UTF-8', 'ASCII', "byte")
```

# Sample a specific number of lines

Next, given the large number of lines within the files, 0,1% of the data will be sampled. The samples of which file will be combined into one document, which will be converted to corpus. Corpora are collections of documents containing (natural language) text. 
```{r message=FALSE, warning=FALSE}
set.seed(333)

# twitter
twitter_sample <- sample(twitter_clean, length(twitter_clean)*0.001)

# blogs
blogs_sample <- sample(blogs_clean, length(blogs_clean)*0.001)

# news
news_sample <- sample(news_clean, length(news_clean)*0.001)

all <- c(twitter_sample,blogs_sample,news_sample)
all_corpus <- VCorpus(VectorSource(all))

rm(twitter_clean,twitter_raw,twitter_sample)
rm(blogs_clean,blogs_raw,blogs_sample)
rm(news_clean,news_raw,news_sample)
```

## Text Mining

All characteres that can't agreggate any meaning for the Natural Language processing that the corpus might contain must be cleaned. 
```{r message=FALSE, warning=FALSE}
all_corpus <- tm_map(all_corpus, content_transformer(tolower)) ## Convert to lower case
all_corpus <- tm_map(all_corpus, removePunctuation) ## Remove all punctuatins
all_corpus <- tm_map(all_corpus, removeNumbers) ## Remove all numbers 
all_corpus <- tm_map(all_corpus, stripWhitespace) ## Remove whitespace
```

## Removing stopwords

This includes words that are important for grammar structure but not the meaning itself, for example some auxiliary verbs, adverbs and pronoums.
```{r message=TRUE, warning=FALSE}
stopwords("en") # retrieves English stopwords from tm database

## By looking at the above list it is possible to see if there are more stopwords that  can also be removed and were not included
extra_stopwords <- c("just", "dont", "ive", "still")

all_corpus<- tm_map(all_corpus, removeWords, stopwords("en")) ## Remove database stopwords
all_corpus<- tm_map(all_corpus, removeWords, extra_stopwords) ## Remove extra stopwords
```

## Tokeninzation

Up to now, the data has been sampled and some characters have been removed. However, all the inputs and output we have been working on were strings. Even the word counting was perfomed on strings by means of a word break iterator. Now, it is time to perform tokenization of the data. *Tokenization is the process of demarcating and possibly classifying sections of a string of input characters* (Extracted from Wikipedia). By doing so, strings will be discriminated into separted words.

## Creating functions for the uni-,bi- and tri- tokeninzation of the data

**The main goal in working with this data is to create a Shiny app to predict the next word** from the one (*unigram*), two (*bigram*) and three (*trigram*) previous words.

**From Package RWeka help files**: *"NGramTokenizer splits strings into n-grams with given minimal and maximal numbers of grams."*

```{r}
bi_tokenizer <- function(x){
                    NGramTokenizer(x, Weka_control(min = 2, max = 2))}
tri_tokenizer <-function(x){
                    NGramTokenizer(x, Weka_control(min = 3, max = 3))}
```

## Create Term Document Matrices 

Constructs or coerces to a term-document matrix or a document-term matrix.
```{r}
uni_tdm <- TermDocumentMatrix(all_corpus)
bi_tdm <- TermDocumentMatrix(all_corpus, control = list(tokenize = bi_tokenizer))
tri_tdm <-TermDocumentMatrix(all_corpus, control = list(tokenize = tri_tokenizer))
```

## Frequency of words

Counting n-gram frequencies and sorting them in decresing order, then storing the results into a data frame. For that, TermDocumentMatrix() output, which is a list, must be coerced to a matrix, so that, the frequency of each word can be summed by rowSums() and the data can be arranged in decresing order by sort(). After that, the output, which is a named number, can be used to create a data frame with the variable "names" storing the words and "freq" storing the frequency of each word.
```{r}
uni_matrix <- as.matrix(uni_tdm)
bi_matrix <- as.matrix(bi_tdm)
tri_matrix <- as.matrix(tri_tdm)

uni_matrix <- sort(rowSums(uni_matrix),decreasing=TRUE)
bi_matrix <- sort(rowSums(bi_matrix),decreasing=TRUE)
tri_matrix <- sort(rowSums(tri_matrix),decreasing=TRUE)

uni_matrix_df <- data.frame(word = names(uni_matrix),freq=uni_matrix, row.names = 1:length(uni_matrix))
bi_matrix_df <- data.frame(word = names(bi_matrix),freq=bi_matrix, row.names = 1:length(bi_matrix))
tri_matrix_df <- data.frame(word = names(tri_matrix),freq=tri_matrix, row.names = 1:length(tri_matrix))
```

# Plots

Histograms with the 25 most frequent n-grams
```{r}
colors<-rep(c("turquoise1","coral1","aquamarine1","khaki1","hotpink1"), times=1, each=5)

par(mfrow=c(1,3))

barplot(uni_matrix_df$freq[1:25], las = 2, names.arg = uni_matrix_df$word[1:25],
        col =colors, main =" 25 most frequent unigrams", 
        ylab = "1-gram frequencies", cex.names = 0.8)

barplot(bi_matrix_df$freq[1:25], las = 2, names.arg = bi_matrix_df$word[1:25],
        col =colors, main ="25 most frequent bigrams",
        ylab = "2-gram frequencies", cex.names = 0.8)


barplot(tri_matrix_df$freq[1:25], las = 2, names.arg = tri_matrix_df$word[1:25],
        col =colors, main ="25 most frequent trigrams",
        ylab = "3-gram frequencies", cex.names = 0.8)
```

And by wordcloud

```{r message=FALSE, warning=FALSE}
par(mfrow=c(1,3))
wordcloud(all_corpus, max.words = 150, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,colors=brewer.pal(8, "Dark2"))
```

# Plans for  prediction algorithm and Shiny app

A stated above the main goal of this project is to create a predictive model of probable word to follow an user input. This model will be build as a shiny application.



#### End